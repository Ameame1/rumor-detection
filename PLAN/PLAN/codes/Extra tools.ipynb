{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import json5\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T11:46:14.830844396Z",
     "start_time": "2023-05-14T11:46:14.671028887Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 将TreeWeibo中的所有信息整合到一个文件weibo_timedelay.txt中"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "# 遍历所有时间信息文件输出到weibo_timedelay.txt\n",
    "# Specify the folder containing the files\n",
    "folder_path = '/home/ame/rumor/PPA/data/TreeWeibo'\n",
    "\n",
    "# Specify the output file path\n",
    "output_path = '/home/ame/rumor/weibo_dataset/weibo_timedelay.txt'\n",
    "\n",
    "# Iterate through all the files in the folder\n",
    "with open(output_path, 'w') as out_file:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Perform the operation on the file\n",
    "            with open(file_path, 'r') as in_file:\n",
    "                lines = in_file.readlines()\n",
    "                lines = [line for line in lines if line.strip() and len(line.split()) >= 2 and line.split()[1] == '0'] # Added a check for lines with at least two items\n",
    "\n",
    "                out_file.writelines(lines)\n",
    "# 3536975887764280\t0\t3536982095687768\t24.68"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T08:16:48.847000Z",
     "end_time": "2023-04-17T08:16:49.892698Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 将原数据与评论数据整合 weibo_id_text.txt + weibo_timedelay.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### weibo_id_text.txt 所有评论信息的内容 weibo_timedelay.txt 所有评论信息的原数据编号 评论数据编号 评论回复原数据时间"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "#首先weibo_id_text.txt文件的第二列数据中存在空格，如果不消除空格对后续数据处理有很大有影响\n",
    "input_file_path = \"weibo_id_text.txt\"\n",
    "output_file_path = \"weibo_id_text2.txt\"\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for line in input_file:\n",
    "        columns = line.strip().split(\"\\t\")\n",
    "        if len(columns) >= 2:\n",
    "            columns[1] = columns[1].replace(\" \", \"\")\n",
    "            output_file.write(\"\\t\".join(columns) + \"\\n\")\n",
    "\n",
    "# Read weibo_id_text.txt file and create a dictionary\n",
    "with open('weibo_id_text2.txt', 'r') as file:\n",
    "    weibo_id_text = {}\n",
    "    for line in file:\n",
    "        line_parts = line.strip().split(maxsplit=1)\n",
    "        if len(line_parts) == 2:\n",
    "            weibo_id_text[line_parts[0]] = line_parts[1]\n",
    "\n",
    "# Read weibo_timedelay.txt and replace the response text number\n",
    "with open('weibo_timedelay.txt', 'r') as file:\n",
    "    weibo_time_delay = [line.strip().split() for line in file if len(line.strip().split()) == 4]\n",
    "\n",
    "for i, row in enumerate(weibo_time_delay):\n",
    "    if row[2] in weibo_id_text:\n",
    "        weibo_time_delay[i][2] = weibo_id_text[row[2]]\n",
    "\n",
    "# Write the updated data to a new file\n",
    "with open('weibo_id_text_merge1.txt', 'w') as file:\n",
    "    for row in weibo_time_delay:\n",
    "        file.write(' '.join(row) + '\\n')\n",
    "\n",
    "#这个时候数据会格式会出现错误，只显示一列\n",
    "with open('weibo_id_text_merge1.txt', 'r') as input_file, open('weibo_id_text_merge2.txt', 'w') as output_file:\n",
    "    for line in input_file:\n",
    "        columns = line.split()\n",
    "        output_file.write('\\t'.join(columns) + '\\n')\n",
    "\n",
    "#第二列的0为为无用数据，删除\n",
    "\n",
    "input_file_path = \"weibo_id_text_merge2.txt\"\n",
    "output_file_path = \"weibo_id_text_merge3.txt\"\n",
    "\n",
    "# Read the input file and remove the second column\n",
    "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "    for line in input_file:\n",
    "        # Split the line into columns\n",
    "        columns = line.split()\n",
    "\n",
    "        # Remove the second column\n",
    "        del columns[1]\n",
    "\n",
    "        # Write the modified line to the output file\n",
    "        output_file.write(' '.join(columns) + '\\n')\n",
    "\n",
    "#同样会出现数据格式错误变为1列，我们将其修复\n",
    "with open('weibo_id_text_merge3.txt', 'r') as input_file, open('weibo_id_text_merge4.txt', 'w') as output_file:\n",
    "    for line in input_file:\n",
    "        columns = line.split()\n",
    "        output_file.write('\\t'.join(columns) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:57:26.990927Z",
     "end_time": "2023-04-24T23:57:39.762663Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 再整合进原数据内容和信息类别label: weibo.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Read weibo_timedelay_modified1.txt\n",
    "with open('weibo_id_text_merge4.txt', 'r') as f:\n",
    "    weibo_timedelay = [line.strip().split() for line in f]\n",
    "\n",
    "# Create a set of metadata ids from weibo_timedelay\n",
    "metadata_ids = set(data[0] for data in weibo_timedelay)\n",
    "\n",
    "# Read weibo.txt and filter by metadata id\n",
    "with open('weibo.txt', 'r') as f:\n",
    "    weibo = [line.strip().split('\\t') for line in f if line.strip().split('\\t')[0] in metadata_ids]\n",
    "\n",
    "# Create a dictionary mapping metadata id to metadata content and label\n",
    "metadata_dict = {data[0]: (data[1], 0 if data[2] == 'false' else 1) for data in weibo}\n",
    "\n",
    "# Add metadata content column and label column to weibo_timedelay\n",
    "weibo_timedelay_with_metadata = [\n",
    "    [data[0], metadata_dict.get(data[0], ('', ''))[0], metadata_dict.get(data[0], ('', ''))[1], data[1], data[2]]\n",
    "    for data in weibo_timedelay\n",
    "]\n",
    "\n",
    "# Write weibo_timedelay_with_metadata to a new file\n",
    "with open('weibo_id_text_merge5.txt', 'w') as f:\n",
    "    for data in weibo_timedelay_with_metadata:\n",
    "        f.write('\\t'.join(map(str, data)) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:11:43.382111Z",
     "end_time": "2023-04-25T00:11:51.183031Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 将txt文件转换为PLAN数据格式的json文件\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "\n",
    "# Read weibo_timedelay_modified2.txt using pandas\n",
    "df = pd.read_csv('weibo_id_text_merge5.txt', sep='\\t', header=None, names=['metadata', 'content_metadata', 'label', 'content_reply', 'time_reply'])\n",
    "\n",
    "# Remove rows with missing values\n",
    "df.dropna(subset=['metadata', 'label', 'time_reply'], inplace=True)\n",
    "\n",
    "# Convert 'metadata' and 'label' columns to integer data type\n",
    "df['metadata'] = df['metadata'].astype(int)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Group the data by the 'metadata' column and create the desired JSON structure\n",
    "grouped_data = {}\n",
    "for index, row in df.iterrows():\n",
    "    metadata, content_metadata, label, content_reply, time_reply = row\n",
    "    if metadata not in grouped_data:\n",
    "        grouped_data[metadata] = {\n",
    "            \"id_\": metadata,\n",
    "            \"label\": label,\n",
    "            \"tweets\": [content_metadata],\n",
    "            \"time_delay\": [0],\n",
    "            \"structure\": [],\n",
    "        }\n",
    "    else:\n",
    "        grouped_data[metadata][\"tweets\"].append(content_reply)\n",
    "        grouped_data[metadata][\"time_delay\"].append(time_reply)\n",
    "\n",
    "# Write the JSON data to a file, with each object on a single line\n",
    "with open('weibo_full.json', 'w') as file:\n",
    "    for data in grouped_data.values():\n",
    "        json.dump(data, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:38:48.951943Z",
     "end_time": "2023-04-25T00:39:24.906447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 统计新生成的json文件的行数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4595 elements in the JSON file.\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'weibo_full.json'\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    line_count = sum(1 for _ in file)\n",
    "\n",
    "print(f'There are {line_count} elements in the JSON file.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:40:14.329662Z",
     "end_time": "2023-04-25T00:40:14.525841Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 统计txt文件不相同元素的个数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different elements in the first column is: 4662\n"
     ]
    }
   ],
   "source": [
    "file_name = 'weibo_timedelay_modified2_filtered.txt'\n",
    "\n",
    "unique_elements = set()\n",
    "with open(file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        first_column = line.strip().split()[0]\n",
    "        unique_elements.add(first_column)\n",
    "\n",
    "print(f'The number of different elements in the first column is: {len(unique_elements)}')\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:18:39.247639Z",
     "end_time": "2023-04-25T00:18:41.419212Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 统计txt文件列数(防止数据混乱)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'weibo_id_text_merge5.txt' has 5 columns.\n"
     ]
    }
   ],
   "source": [
    "# define the filename and delimiter\n",
    "filename = \"weibo_id_text_merge4.txt\"\n",
    "delimiter = \"\\t\"  # change to \",\" for a CSV file\n",
    "\n",
    "# read the first row of the file and count the number of delimiter characters\n",
    "with open(filename, \"r\") as file:\n",
    "    first_row = file.readline().strip()\n",
    "    num_columns = first_row.count(delimiter) + 1  # add 1 to account for the last column\n",
    "\n",
    "# output the result\n",
    "print(f\"The file '{filename}' has {num_columns} columns.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:16:52.908529Z",
     "end_time": "2023-04-24T23:16:52.916012Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 按70% 15% 15%分配数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the JSON data from the file\n",
    "with open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_full_data_repair.json', 'r') as file:\n",
    "    json_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(json_data)\n",
    "\n",
    "# Split the data into a training set, a test set 1, and a test set 2 with a 70:15:15 ratio\n",
    "train_index = int(len(json_data) * 0.7)\n",
    "test1_index = int(len(json_data) * 0.85)\n",
    "train_data = json_data[:train_index]\n",
    "test1_data = json_data[train_index:test1_index]\n",
    "test2_data = json_data[test1_index:]\n",
    "\n",
    "# Write the training set to a file\n",
    "with open('weibo_train_repair.json', 'w') as file:\n",
    "    for data in train_data:\n",
    "        json.dump(data, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n",
    "\n",
    "# Write test set 1 to a file\n",
    "with open('weibo_test1_repair.json', 'w') as file:\n",
    "    for data in test1_data:\n",
    "        json.dump(data, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n",
    "\n",
    "# Write test set 2 to a file\n",
    "with open('weibo_test2_repair.json', 'w') as file:\n",
    "    for data in test2_data:\n",
    "        json.dump(data, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:02:23.865360Z",
     "end_time": "2023-04-25T15:02:24.567236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 统计所有json数据中的time_delay[]包含的最大元素值和包含最多的元素个数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number in all time_delay arrays is 98.\n",
      "The maximum number of elements in all time_delay arrays is 229.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the maximum element count and maximum number\n",
    "max_element_count = 0\n",
    "max_number = 0\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('/home/ame/rumor/PLAN/codes/pheme.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        time_delay = item['time_delay']\n",
    "\n",
    "        # Update the maximum number if a larger value is found\n",
    "        current_max_number = max(time_delay)\n",
    "        max_number = max(max_number, current_max_number)\n",
    "\n",
    "        # Update the maximum element count\n",
    "        max_element_count = max(max_element_count, len(time_delay))\n",
    "\n",
    "print(f'The maximum number in all time_delay arrays is {max_number}.')\n",
    "print(f'The maximum number of elements in all time_delay arrays is {max_element_count}.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T04:37:15.301982Z",
     "end_time": "2023-04-19T04:37:15.364351Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 更改微博数据集 中的time_dealy[] 最大为100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# Initialize the output list\n",
    "output_data = []\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        time_delay = item['time_delay']\n",
    "\n",
    "        # Cap numbers greater than 100 to 100\n",
    "        capped_time_delay = [min(x, 100) for x in time_delay]\n",
    "\n",
    "        # Update the time_delay array in the current JSON object\n",
    "        item['time_delay'] = capped_time_delay\n",
    "\n",
    "        # Add the modified JSON object to the output list\n",
    "        output_data.append(item)\n",
    "\n",
    "# Write the modified JSON objects to a new file\n",
    "with open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_full_change_time_delay.json', 'w', encoding='utf-8') as f:\n",
    "    for item in output_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T05:51:11.844144Z",
     "end_time": "2023-04-19T05:51:13.758224Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 不再是将大于100的数据变为100而是删除，同时删除对应的tweet[]中的数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize the output list\n",
    "output_data = []\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('weibo_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        time_delay = item['time_delay']\n",
    "\n",
    "        # Remove items in the time_delay and tweets arrays where time_delay > 100\n",
    "        cleaned_time_delay = []\n",
    "        cleaned_tweets = []\n",
    "        for i, time_val in enumerate(time_delay):\n",
    "            try:\n",
    "                time_val_float = float(time_val)  # Attempt to convert time_val to float\n",
    "                if time_val_float <= 100:\n",
    "                    cleaned_time_delay.append(time_val_float)\n",
    "                    cleaned_tweets.append(item['tweets'][i])\n",
    "            except ValueError:\n",
    "                # Skip the value if it cannot be converted to float\n",
    "                continue\n",
    "\n",
    "        # Update the time_delay and tweets arrays in the current JSON object\n",
    "        item['time_delay'] = cleaned_time_delay\n",
    "        item['tweets'] = cleaned_tweets\n",
    "\n",
    "        # Add the modified JSON object to the output list\n",
    "        output_data.append(item)\n",
    "\n",
    "# Write the modified JSON objects to a new file\n",
    "with open('weibo_full_del.json', 'w', encoding='utf-8') as f:\n",
    "    for item in output_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:56:05.573768Z",
     "end_time": "2023-04-25T00:56:06.945108Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 统计weibo数据中的label参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of label 0: 2281\n",
      "Count of label 1: 2315\n"
     ]
    }
   ],
   "source": [
    "# Initialize label counters\n",
    "label_0_count = 0\n",
    "label_1_count = 1\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('weibo_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        label = item['label']\n",
    "\n",
    "        # Update the label counters\n",
    "        if label == 0:\n",
    "            label_0_count += 1\n",
    "        elif label == 1:\n",
    "            label_1_count += 1\n",
    "\n",
    "# Print the count of each label\n",
    "print(f\"Count of label 0: {label_0_count}\")\n",
    "print(f\"Count of label 1: {label_1_count}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T00:39:24.947868Z",
     "end_time": "2023-04-25T00:39:25.333820Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 删除微博数据集 中的time_dealy[]的元素\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Read the JSON file line by line and process the objects\n",
    "with open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_test1.json', 'r', encoding='utf-8') as input_file, \\\n",
    "        open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_test1_no_time_dealy.json', 'w', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        item['time_delay'] = []\n",
    "\n",
    "        # Write the modified JSON object to the output file with ensure_ascii=False\n",
    "        json.dump(item, output_file, ensure_ascii=False)\n",
    "        output_file.write('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T04:55:56.678824Z",
     "end_time": "2023-04-19T04:55:56.837710Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 拆分json数据\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "with open('/home/ame/rumor/PLAN/codes/data/weibo/weibo_full_del.json', 'r',encoding='utf-8') as f:\n",
    "    data = f.read().strip().split('\\n')\n",
    "\n",
    "for i, obj in enumerate(data):\n",
    "    with open(f'weibo_full_change_time_dealy_{i}.json', 'w') as f_out:\n",
    "        json.dump(json.loads(obj), f_out, separators=(',', ':'),ensure_ascii=False)\n",
    "        f_out.write('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T01:07:18.814941Z",
     "end_time": "2023-04-25T01:07:19.762623Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 查找错误json文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check_file_path = os.path.join('/home/ame/rumor/PLAN/codes', 'weibo_check.py')\n",
    "%run $check_file_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T18:36:58.921764Z",
     "end_time": "2023-04-25T18:37:08.127938Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 合并json文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging files: 100%|██████████| 4594/4594 [02:13<00:00, 34.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_line(line):\n",
    "    return ''.join(c for c in line if not c.isspace() or c.isprintable())\n",
    "\n",
    "def read_json_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                line = clean_line(line.strip())\n",
    "                if not line:\n",
    "                    continue\n",
    "                yield json5.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping malformed JSON object in {file_path}: {line}\")\n",
    "\n",
    "# Rest of the code remains the same\n",
    "\n",
    "\n",
    "def write_json_lines(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for item in data:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(f\"{json_line}\\n\")\n",
    "\n",
    "def merge_json_files(directory_path, output_file):\n",
    "    merged_data = []\n",
    "\n",
    "    # Iterate over all files in the specified directory\n",
    "    for file_name in tqdm(os.listdir(directory_path), desc=\"Merging files\"):\n",
    "        # Check if the file is a JSON file\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Read the JSON lines and merge them into the merged_data list\n",
    "            for json_line in read_json_lines(file_path):\n",
    "                merged_data.append(json_line)\n",
    "\n",
    "    # Write the merged data to the output file\n",
    "    write_json_lines(output_file, merged_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_directory = '/home/ame/rumor/PLAN/codes/data/weibo/test2'\n",
    "    output_file = '/home/ame/rumor/PLAN/codes/data/weibo/weibo_full_data_repair.json'\n",
    "    merge_json_files(input_directory, output_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T02:27:53.689189Z",
     "end_time": "2023-04-25T02:30:07.864431Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 我们将微博数据集分成5份，选取当中的一份按照训练集：测试集1：测试集2为70：15：15的比例进行测试\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def read_json_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line.strip()) for line in file]\n",
    "    return data\n",
    "\n",
    "def write_json_lines(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for item in data:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(f\"{json_line}\\n\")\n",
    "\n",
    "def split_dataset(dataset, ratios):\n",
    "    random.shuffle(dataset)\n",
    "    total_length = len(dataset)\n",
    "    splits = []\n",
    "    current_index = 0\n",
    "    for ratio in ratios:\n",
    "        split_length = int(total_length * ratio)\n",
    "        splits.append(dataset[current_index:current_index + split_length])\n",
    "        current_index += split_length\n",
    "    return splits\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_file = '/home/ame/rumor/PLAN/codes/data/weibo/weibo_full_data_repair.json'\n",
    "    output_directory = '/home/ame/rumor/PLAN/codes/data/weibo'\n",
    "\n",
    "    dataset = read_json_lines(dataset_file)\n",
    "\n",
    "    num_splits = 5\n",
    "    equal_parts = [dataset[i::num_splits] for i in range(num_splits)]\n",
    "\n",
    "    for i, part in enumerate(equal_parts):\n",
    "        train, test1, test2 = split_dataset(part, [0.7, 0.15, 0.15])\n",
    "        write_json_lines(os.path.join(output_directory, f'weibo_split_{i}_train.json'), train)\n",
    "        write_json_lines(os.path.join(output_directory, f'weibo_split_{i}_test1.json'), test1)\n",
    "        write_json_lines(os.path.join(output_directory, f'weibo_split_{i}_test2.json'), test2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T02:31:58.257240Z",
     "end_time": "2023-04-25T02:31:58.684767Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创造weibo数据集，纠正label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open('/home/ame/rumor/weibo/weibo_source_text.txt', 'r', encoding='utf-8') as input_file, open('weibo_source_text.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        columns = line.strip().split('\\t')\n",
    "        if columns[2] == 'non-rumor':\n",
    "            columns[2] = 'true'\n",
    "        output_file.write('\\t'.join(columns) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:25:43.317224Z",
     "end_time": "2023-04-25T15:25:43.554815Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 统计twitter 15/16label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of label 0: 836\n",
      "Count of label 1: 297\n",
      "Count of label 2: 0\n",
      "Count of label 3: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize label counters\n",
    "label_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('/home/ame/rumor/PLAN/codes/data/twitter15_16/twitter15/split_data/structure_v2/split_0/train_unique_w_structure_v2_modified.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        label = item['label']\n",
    "\n",
    "        # Update the label counters\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Print the count of each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Count of label {label}: {count}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:06:49.626708275Z",
     "start_time": "2023-05-14T12:06:49.514230231Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 改变twitter15  的label\n",
    "#string_to_id = {\"false\" : 0,\n",
    "                \"true\" : 1,\n",
    "                \"unverified\" : 2,\n",
    "                \"non-rumor\" : 3}\n",
    "\n",
    "false-rumor, true-rumor, unverified all are rumor, non-rumor is non-rumor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the input JSON file for reading and create a new output JSON file for writing\n",
    "with open('/home/ame/rumor/PLAN/codes/data/pheme/split_data/structure/split_1/test_w_structure.json', 'r') as infile, open('/home/ame/rumor/PLAN/codes/data/pheme/split_data/structure/split_0/test2_w_structure_modified.json', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        # Ignore empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        label = item['label']\n",
    "\n",
    "        # Update the label values\n",
    "        if label in [1,2]:\n",
    "            item['label'] = 0\n",
    "        elif label == 0:\n",
    "            item['label'] = 1\n",
    "\n",
    "        # Write the modified item to the output JSON file\n",
    "        json.dump(item, outfile)\n",
    "        outfile.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T20:16:09.580393284Z",
     "start_time": "2023-05-09T20:16:09.474067932Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
